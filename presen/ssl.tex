%% LyX 2.2.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{beamer}
\usepackage{mathptmx}
\usepackage{helvet}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\setcounter{tocdepth}{1}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stackrel}
\usepackage{graphicx}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
 % this default might be overridden by plain title style
 \newcommand\makebeamertitle{\frame{\maketitle}}%
 % (ERT) argument for the TOC
 \AtBeginDocument{%
   \let\origtableofcontents=\tableofcontents
   \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
   \def\gobbletableofcontents#1{\origtableofcontents}
 }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage[spanish]{babel}
\uselanguage{spanish}
\languagepath{spanish}
\deftranslation[to=spanish]{Theorem}{Teorema}
\deftranslation[to=spanish]{theorem}{teorema}
\deftranslation[to=spanish]{Definition}{Definición}
\deftranslation[to=spanish]{definition}{definición}
\usetheme{MasterCeura}	% tree outline on top, plainish white
\setbeamercovered{transparent}
\usepackage{multimedia}

\makeatother

\begin{document}
\title[SSL]{Aprendizaje semi-supervisado (SSL)}
\subtitle{Teoría y casos prácticos de SSL}
\author[J. M. Cuadra]{José Manuel Cuadra Troncoso}
\date{}
%\date{\today}
%\institute{\url{jmcuadra@dia.uned.es}\\\url{http://www.ia.uned.es/personal/jmcuadra/}}
\begin{frame}[plain,t]
\titlepage
\end{frame}

\AtBeginSection[{\tableofcontents[currentsection]}]{

  \frame<beamer>{ 

    \frametitle{Índice}   

    \tableofcontents[currentsection,hideothersubsections] 

  }

}
\AtBeginSubsection[{\tableofcontents[currentsection]}]{

  \frame<beamer>{ 

    \frametitle{Índice}   

    \tableofcontents[currentsection,currentsubsection,sectionstyle=show/shaded, subsectionstyle=show/shaded/hide] 

  }

}
\begin{frame}{Índice}

\tableofcontents{}

\setcounter{tocdepth}{2}
\end{frame}

\section{Introducción a SSL}

\subsection{Entre el aprendizaje no supervisado y el supervisado}
\begin{frame}{Aprendizaje no supervisado}
\begin{itemize}
\item <1->El objetivo del aprendizaje no supervisado es encontrar una estructura
interesante en un conjunto de ejemplos $X=\{\mathbf{x}_{1,}\,\mathbf{x}_{2,}\,....,\,\mathbf{x}_{n}\}$
extraídas de una población $\mathcal{X}$.
\begin{itemize}
\item <2->Más precisamente encontrar una f.d.p que pueda haber generado
$X$.
\item <3->Este objetivo se extiende a estimación de cuantiles, clustering,
detección de outliers y reducción de dimensionalidad.
\end{itemize}
\end{itemize}
\end{frame}
%
\begin{frame}{Aprendizaje supervisado}
\begin{itemize}
\item <1->El objetivo del aprendizaje supervisado es aprender un mapeo
$f$ de $X$ en $Y=\{y_{1},\,y_{2},\,...,\,y_{n}\}$, donde $y_{i}$
son las etiquetas asociadas a los ejemplos $x_{i}$ extraídas de una
población $\mathcal{Y}$.
\begin{itemize}
\item <2->Los pares $(\mathbf{x}_{i},\,y_{i})$ se supone que se seleccionan
de manera i.i.d. en la población $\mathcal{X}\times\mathcal{Y}$.
\item <3->Este objetivo se extiende a regresión, detección de outliers
y de novedad.
\end{itemize}
\end{itemize}
\end{frame}
%

\subsection{Datos etiquetados}
\begin{frame}{Pros y contras de los datos etiquetados}
\begin{exampleblock}{Pros}
\begin{itemize}
\item Con relativamente pocos datos se puede entrenar.
\end{itemize}
\end{exampleblock}
%
\begin{alertblock}{Contras}
\begin{itemize}
\item Las etiquetas suelen necesitar expertos anotadores, llevan tiempo
y dinero.
\item Las etiquetas pueden necesitar dispositivos especializados.
\end{itemize}
\end{alertblock}
\begin{itemize}
\item <2->Los datos no etiquetados suelen presentar características opuestas.
\end{itemize}
\end{frame}
%
\begin{frame}{Ejemplo de obtención de etiquetas}

\framesubtitle{Anotación de imágenes médicas}
\begin{center}
\includegraphics[scale=0.7]{figs/slices}
\par\end{center}

\end{frame}
%
\begin{frame}{Uso de juegos para etiquetado}
\begin{itemize}
\item Juegos de computación basados en humanos o juegos con un propósito
(GWAP).
\item Para abordar problemas que las computadoras aún no pueden:
\begin{itemize}
\item Etiquetado de imágenes.
\item Anotación de obras de arte, literatura...
\item Genética.
\item Web semántica ...
\end{itemize}
\item \url{https://en.wikipedia.org/wiki/Human-based_computation_game}
\end{itemize}
\end{frame}
%

\subsection{Aprendizaje semi-supervisado y transductivo}
\begin{frame}{Aprendizaje semi-supervisado}
\begin{itemize}
\item <1->Clasificación semi-supervisada:
\begin{itemize}
\item <2->Usar $l$ datos etiquetados $\left\{ (\mathbf{x}_{i},\,y_{i})\right\} _{i=1}^{l}$
y $u$ no etiquetados $\left\{ \mathbf{x}_{j}\right\} _{j=l+1}^{l+u}$
. Normalmente $u\gg l$.
\item <2->Objetivo: obtener un mejor clasificador que con datos etiquetados
solo. Extensión de SL.
\item <2->Hay métodos generativos (probabilísticos) y discriminativos (no
probabilísticos).
\end{itemize}
\item <3->Clustering con restricciones:
\begin{itemize}
\item <3->Usar $n$ datos no etiquetados $\left\{ \mathbf{x}_{j}\right\} _{j=l}^{n}$
y restricciones p. ej. dos puntos están en el mismo cluster (must-links)
o no (cannot-links).
\item <3->Objetivo: obtener un mejor agrupamiento que con datos no etiquetados
solo. Extensión de USL.
\end{itemize}
\end{itemize}
\end{frame}
%
\begin{frame}{SSL vs aprendizaje transductivo}
\begin{block}{SSL inductivo}

Dados $\left\{ (\mathbf{x}_{i},\,y_{i})\right\} _{i=1}^{l}$ y $\left\{ \mathbf{x}_{j}\right\} _{j=l+1}^{l+u}$
aprender $f\,:\,\mathcal{X}\,\longrightarrow\,\mathcal{Y}$ , se espera
que $f$ realice buenas \textcolor{red}{predicciones sobre datos futuros}.
\end{block}
\begin{block}{Aprendizaje transductivo}

Dados $\left\{ (\mathbf{x}_{i},\,y_{i})\right\} _{i=1}^{l}$ y $\left\{ \mathbf{x}_{j}\right\} _{j=l+1}^{l+u}$
aprender $f\,:\,X^{l+u}\,\longrightarrow\,Y^{l+u}$ , se espera que
$f$ realice buenas \textcolor{red}{predicciones solo en la muestra
de entrenamiento}.
\end{block}
\end{frame}
%

\subsection{¿Cuándo puede funcionar el SSL?}
\begin{frame}{¿Cuándo puede funcionar el SSL?}
\begin{block}{¿Tiene sentido el SSL?}

Precisando: en comparación con un algoritmo supervisado ¿se puede
esperar mayor precisión empleando datos sin etiquetar?
\end{block}
\begin{itemize}
\item <2->Sí, pero la distribución de ejemplo debe ser relevante para el
problema de clasificación planteado.
\begin{itemize}
\item <3->El conocimiento sobre $p(\mathbf{x})$ que aportan los datos
no etiquetados debe tener información útil sobre $p(y|\mathbf{x})$.
\end{itemize}
\item <4->Para esto deben cumplirse ciertas hipótesis: suavidad/continuidad,
agrupamiento y de variedad(manifold).
\end{itemize}
\end{frame}
%
\begin{frame}{Hipótesis de suavidad/continuidad semi-supervisada}
\begin{columns}

\column{0.5\textwidth}
\begin{itemize}
\item <1->Si dos puntos $\mathbf{x}_{1},\,\mathbf{x}_{2}$ en una región
de alta densidad están cerca, sus etiquetas $y_{1},\,y_{2}$ deberían
ser iguales.
\item <2->La frontera de decisión en región de baja densidad.
\item <3->Generalizar a un posible conjunto infinito de casos a partir
de un conjunto finito de entrenamiento.
\end{itemize}

\column{0.5\textwidth}
\begin{center}
\includegraphics<1>[scale=0.5]{figs/smooth-asumpt1.pdf}\includegraphics<2->[scale=0.5]{figs/smooth-asumpt1-2.pdf}
\par\end{center}

\end{columns}

\end{frame}
%
\begin{frame}{Hipótesis de agrupamiento}
\begin{itemize}
\item <1->Si dos puntos $\mathbf{x}_{1},\,\mathbf{x}_{2}$ pertenecen al
mismo cluster, sus etiquetas $y_{1},\,y_{2}$ deberían ser iguales.
Aunque datos con la misma etiqueta pueden pertenecer a distintos clusters.
Caso particular de la hipótesis de suavidad.
\item <2->Los datos no etiquetados pueden ayudar a precisar las fronteras
de los clusters.
\end{itemize}
\begin{center}
\includegraphics<2->[scale=0.35]{figs/cluster-asumpt1-5.pdf}\llap{\includegraphics<3->[scale=0.35]{figs/cluster-asumpt-10.pdf}}\llap{\includegraphics<4->[scale=0.35]{figs/cluster-asumpt-15.pdf}}\llap{\includegraphics<5->[scale=0.35]{figs/cluster-asumpt-20.pdf}}
\par\end{center}

\end{frame}
%
\begin{frame}{HIpótesis de variedad (manifold)}
\begin{itemize}
\item <1->Una variedad es un conjunto localmente homeomórfico a un espacio
euclídeo.
\begin{itemize}
\item <2->1D: recta, círculo, pero no un 8. En 2D: plano, esfera, toro.
\end{itemize}
\item <3->Los datos están en una variedad de una dimensión mucho más pequeña
que la del espacio de entrada (evitar la maldición de la dimensionalidad).
\begin{itemize}
\item <4->La voz humana se controla con 4 cuerdas vocales, no es necesario
modelar el espacio de todas las señales acústicas.
\item <4->La expresión facial se controla con unos pocos músculos, no es
necesario modelar el espacio de todas las imágenes.
\end{itemize}
\end{itemize}
\end{frame}
%
\begin{frame}{Principio de Vapnik}
\begin{definition}[Principio de Vapnik]
Cuando se trata de resolver un problema no debería resolverse, como
paso intermedio, un problema más difícil.
\end{definition}

\end{frame}
%

\section{Algoritmos para SSL}

\subsection{Self-training}
\begin{frame}{Algoritmo básico}
\begin{enumerate}
\item <1->Entrenar un clasificador supervisado $f$ con el conjunto de
datos etiquetados $L$.
\item <2->Usar el clasificador para clasificar el conjunto de datos no
etiquetados $U$.
\item <3->Pasar datos de $U$ a $L$.
\item <4->Repetir los pasos 1, 2 y 3 hasta que $U$ quede vacío.
\end{enumerate}
\end{frame}
%
\begin{frame}{Características}
\begin{itemize}
\item <1->Es el método más simple de SSL y es usado a menudo.
\item <2->Es un método wrapper: envuelve un método SL.
\item <3->La elección del clasificador $f$ está abierta.
\item <4->Los errores cometidos por $f$ tienden a reforzarse.
\begin{itemize}
\item Como solución se pueden desetiquetar muestras con fiabilidad por debajo
de un umbral.
\end{itemize}
\item <5->La convergencia a una solución depende del caso.
\end{itemize}
\end{frame}
%
\begin{frame}{Variaciones al método básico}
\begin{itemize}
\item <1->Añadir a $L$ solo los datos más fiables en cada iteración.
\item <2->Añadir a $L$ todos los datos, habría una sola iteración.
\item <3->Añadir a $L$ todos los datos ponderando según su fiabilidad,
habría una sola iteración.
\end{itemize}
\end{frame}
%
\begin{frame}{Ejemplo exitoso con 1-NN}
\begin{columns}
%

\column{0.5\textwidth}
\begin{itemize}
\item <1->Tenemos $L$ y $U$.
\begin{enumerate}
\item <2->Seleccionamos elemento de $U$ que esté a la mínima distancia
de cualquier elemento de $L$.
\item <3->Le damos la etiqueta del elemento de $L$ más cercano a él y
lo pasamos a $L$. Los empates se deshacen aleatoriamente.
\end{enumerate}
\item <4->Repetimos 1 y 2 hasta vaciar $U$.
\end{itemize}

\column{0.5\textwidth}
\begin{center}
\includegraphics<1->[scale=0.5]{figs/self-training-ok-0.pdf}\llap{\includegraphics<1>[scale=0.5]{figs/self-training-ok-1.pdf}}\llap{\includegraphics<2-3>[scale=0.5]{figs/self-training-ok-2.pdf}}\llap{\includegraphics<2>[scale=0.5]{figs/self-training-ok-3.pdf}}\llap{\includegraphics<3>[scale=0.5]{figs/self-training-ok-4.pdf}}\llap{\includegraphics<4>[scale=0.5]{figs/self-training-ok-5.pdf}}\llap{\includegraphics<5>[scale=0.5]{figs/self-training-ok-10.pdf}}\llap{\includegraphics<6->[scale=0.5]{figs/self-training-ok-15.pdf}}
\par\end{center}

\end{columns}

\end{frame}
%
\begin{frame}{Ejemplo fallido con 1-NN}
\begin{columns}
%

\column{0.5\textwidth}
\begin{itemize}
\item <1->Tenemos $L$ y $U$, pero ahora hay un outlier.
\item <2->Procedemos como en el ejemplo anterior.
\item <3->Un outlier situado en el sitio oportuno puede dar al traste con
la clasificación.
\item <4->El outlier propaga su etiqueta.
\item <5->La clasificación no es correcta.
\end{itemize}

\column{0.5\textwidth}
\begin{center}
\includegraphics<1->[scale=0.5]{figs/self-training-not-ok-0.pdf}\llap{\includegraphics<1>[scale=0.5]{figs/self-training-not-ok-1.pdf}}\llap{\includegraphics<2>[scale=0.5]{figs/self-training-not-ok-5.pdf}}\llap{\includegraphics<3-4>[scale=0.5]{figs/self-training-not-ok-10.pdf}}\llap{\includegraphics<4>[scale=0.5]{figs/self-training-not-ok-12.pdf}}\llap{\includegraphics<5->[scale=0.5]{figs/self-training-not-ok-15.pdf}}
\par\end{center}

\end{columns}

\end{frame}

\subsection{Co-training y modelos multivista}
\begin{frame}{Introducción}
\begin{itemize}
\item <1->Queremos clasificar web en categorías.
\begin{itemize}
\item Creamos dos vistas (conjuntos de características $X=[X_{1},\,X_{2}]$
de cada web: contenido $X_{1}$ y link $X_{2}$.
\item Etiquetamos las vistas de algunas webs según las categorías.
\end{itemize}
\item <2->Entrenamos un clasificador para cada vista y cada uno clasifica
algunos datos no etiquetados
\item <3->Cada clasificador enseña al otro con los datos que ha etiquetado.
\item <4->Como el self-training pero con dos clasificadores que se enseñan
mutuamente.
\end{itemize}
\end{frame}
%
\begin{frame}{Hipótesis del co-training}
\begin{block}{Hipótesis del co-training}
\begin{itemize}
\item <1->La división en vistas $X=[X_{1},\,X_{2}]$ existe.
\item <2->Cada vista es capaz de entrenar un clasificador.
\item <3->Las vistas son condicionalmente independientes dada una clase
$C$ ($X_{1}\,\mathsf{y}\,X_{2}$ no tiene porqué ser independientes
pero si nos dan $C$ sí (peso, vocabulario y edad).
\end{itemize}
\end{block}
\begin{center}
\includegraphics<3->[scale=0.5]{figs/dos-vistas_horz.pdf}
\par\end{center}

\end{frame}
%
\begin{frame}{Algoritmo básico del co-training}
\begin{enumerate}
\item <1->Entrenar dos clasificadores $f_{1}$ a partir de $\left(X_{1}^{l},\,Y_{1}^{l}\right)$,
$f_{2}$ a partir de $\left(X_{2}^{l},\,Y_{2}^{l}\right)$.
\item <2->Clasificar $X^{u}$ usando $f_{1}$ y $f_{2}$ de forma independiente.
\item <3->Co-training a velocidad $k$.
\begin{itemize}
\item <3->Añadir los $k$ datos más fiablemente etiquetados por $f_{1}$
a $\left(X_{2}^{l},\,Y_{2}^{l}\right)$.
\item <3->Añadir los $k$ datos más fiablemente etiquetados por $f_{2}$
a $\left(X_{1}^{l},\,Y_{1}^{l}\right)$.
\end{itemize}
\item <4->Eliminar estos $2k$ datos de $X^{u}$.
\item <5->Repetir los pasos 2, 3 y 4 hasta acabar con $X^{u}$.
\end{enumerate}
\end{frame}
%
\begin{frame}{Pros y contras del co-training}
\begin{exampleblock}{Pros}
\begin{itemize}
\item Es un método wrapper que se puede aplicar a mucho clasificadores supervisados.
\item Es menos sensible a errores que el self-training.
\end{itemize}
\end{exampleblock}
%
\begin{alertblock}{Contras}
\begin{itemize}
\item La división de vistas de forma natural puede no existir.
\item Modelos usando ambas vistas pueden funcionar mejor.
\end{itemize}
\end{alertblock}
\end{frame}
%
\begin{frame}{Variantes del co-training}
\begin{itemize}
\item <1->\textbf{Co-EM (expectation maximization)}: da etiquetas probabilísticas
que pueden cambiar entre iteraciones.
\item <2->\textbf{Co-regularization}: minimiza una función que depende
de la complejidad (normas) de $f_{1}\,\mathsf{y}\,f_{2}$, su acuerdo
en los datos no etiquetados y una función de pérdida evaluada en los
datos etiquetados usando la media entre $f_{1}\,\mathsf{y}\,f_{2}$.
\item <3->\textbf{Co-regression}: usa regresores en lugar de clasificadores,
la fiabilidad de las nuevas etiquetas se estima por la disminución
del MSE.
\item <4->\textbf{Co-clustering}: funciona bajo la hipótesis de que el
agrupamiento real subyacente asignará los puntos correspondientes
en cada vista al mismo grupo.
\end{itemize}
\end{frame}
%
\begin{frame}{Modelos multivista}
\begin{itemize}
\item <1->Son una extensión del co-training en la que se usan más de dos
clasificadores.
\begin{itemize}
\item <2->Se entrenan clasificadores de distintos tipos.
\item <2->Se clasifican los datos no etiquetados con cada clasificador.
\item <2->Se añade la etiqueta votada por la mayoría.
\end{itemize}
\end{itemize}
\end{frame}

\subsection{Self-labeled}
\begin{frame}{Introducción a self-labeled}
\begin{enumerate}
\item <1->Estos métodos engloban al self-training y al co-training/multivista.
\item <2->Características para clasificar:
\begin{enumerate}
\item <2->Mecanismo de adición: 
\begin{enumerate}
\item Incremental:$k$ más fiables en cada iteración.
\item Por lotes: Se decide si un dato cumple el criterio de adición pero
hasta toman todas las decisiones no se etiquetan realmente (todos)
los datos, se puede cambiar de opinión.
\item Corrección: se añaden todos lo que cumplen un criterio, los que se
etiquetan en una iteración pueden perderla en otra.
\end{enumerate}
\item <3->Clasificador simple (p. ej. self-learning) vs. múltiple (p. ej.
co.training).
\item <4->Usar uno o varios algoritmos de aprendizaje.
\item <5->Vista simple o multivista.
\end{enumerate}
\end{enumerate}
\end{frame}
%
\begin{frame}{Taxonomía de métodos self-labeled}
\begin{center}
\includegraphics{figs/self-labeled}
\par\end{center}
\end{frame}

\subsection{Modelos generativos}
\begin{frame}{Definición de modelo generativo}
\begin{definition}[Modelo generativo]
$p(\mathbf{x},\,y)\,=\,p(y)p(\mathbf{x}|y)$ siendo $p(\mathbf{x}|y)$
una distribución de probabilidad compuesta (mixture distribution MM)
identificable.

\medskip{}

Una MM es una distribución de probabilidad en la que los parámetros,
o parte de ellos, son variables aleatorias.

Una familia de distribuciones $\{p_{\theta}\}$ es identificable si
$\theta_{1}\neq\theta_{2}\Longrightarrow p_{\theta_{1}}\neq p_{\theta_{2}}$.
\end{definition}

\begin{itemize}
\item <2->Con una cantidad suficiente de datos no etiquetados se pueden
identificar los componentes de la composición.
\end{itemize}
\end{frame}
%
\begin{frame}{Modelo generativo en SSL}
\begin{itemize}
\item <1->Asumimos que conocemos $p(\mathbf{x},\,y)$ con parámetros $\theta$.
\item <2->La distribución conjunta y la marginal $p(X^{l},\,Y^{l},\,X^{u}|\theta)\,=\,\underset{Y^{u}}{\sum}p(X^{l},\,Y^{l},\,X^{u},\,Y^{u}|\theta)$.
\item <3->Estimar $\theta$ usando MLE o MAP o por métodos bayesianos.
\item <4->Con una cantidad suficiente de datos no etiquetados se pueden
identificar los componentes de la suma.
\end{itemize}
\end{frame}
%
\begin{frame}{Ejemplos de modelos generativos}
\begin{itemize}
\item <1->Mixtura de gaussianas (GMM):
\begin{itemize}
\item Clasificación de imágenes usando MLE.
\end{itemize}
\item <2->Mixtura de multinomiales (Naïve Bayes):
\begin{itemize}
\item Categorización de textos usando MLE.
\end{itemize}
\item <3->Modelos de Markov ocultos (HMM):
\begin{itemize}
\item Reconocimiento de voz usando el algoritmo de Baum y Welch.
\end{itemize}
\end{itemize}
\end{frame}
%
\begin{frame}{Clasificación con GMM usando MLE}

\framesubtitle{La mixtura}
\begin{itemize}
\item <1->Parámetros del modelo: $\theta=\left\{ w_{1},\,w_{2},\,\mu_{1},\,\mu_{2},\,\text{\ensuremath{\Sigma}}_{1},\,\text{\ensuremath{\Sigma}}_{2}\right\} $
proporciones de clases, medias y covarianzas.
\item <1->La GMM: $p(\mathbf{x},\,y)\,=\,\stackrel[i=1]{2}{\sum}w_{i}\mathcal{N}(\mu_{i},\,\Sigma_{i})$.
\item <2->Clasificación: $p(y|\mathbf{x},\theta)=\dfrac{p(\mathbf{x},y|\theta)}{\underset{y'}{\sum}p(\mathbf{x},y'|\theta)}$.
\end{itemize}
\end{frame}
%
\begin{frame}{Clasificación con GMM usando MLE}

\framesubtitle{Clasificación binaria}
\begin{itemize}
\item <1->Usando datos etiquetados
\begin{itemize}
\item $\log p(X_{l},Y_{l}|\theta)=\stackrel[i=1]{l}{\sum}\log p(y_{i}|\theta)p(\mathbf{x}_{i}|y_{i},\theta)$.
\item La MLE es trivial: $w_{i}\rightarrow$frecuencias, $\mu_{i}\rightarrow$medias
muestrales $\Sigma_{i}\rightarrow$covarianzas muestrales para cada
clase.
\end{itemize}
\item <2->Con los datos etiquetados y \textcolor{red}{no etiquetados}:
\begin{itemize}
\item $\log p(X_{l},Y_{l},X_{u}|\theta)=\stackrel[i=1]{l}{\sum}\log p(y_{i}|\theta)p(\mathbf{x}_{i}|y_{i},\theta)+$\\
\hspace*{0.22\paperwidth}${\color{red}\stackrel[i=l+1]{l+u}{\sum}\log\left(\stackrel[y=1]{2}{\sum}p(y|\theta)p(\mathbf{x}_{i}|y,\theta)\right)}$.
\end{itemize}
\item <3->La MLE es difícil, hay variables ocultas ($Y^{u}$).
\begin{itemize}
\item Usaremos el algoritmo iterativo EM para obtener un máximo local.
\end{itemize}
\end{itemize}
\end{frame}
%
\begin{frame}{Clasificación con GMM usando MLE}

\framesubtitle{Aplicación de EM}
\begin{enumerate}
\item <1->Obtenemos una estimación inicial $\theta^{0}=\left\{ w_{1}^{0},\,w_{2}^{0},\,\mu_{1}^{0},\,\mu_{2}^{0},\,\text{\ensuremath{\Sigma}}_{1}^{0},\,\text{\ensuremath{\Sigma}}_{2}^{0}\right\} $
por MLE usando $(X^{l},\,Y^{l})$.
\item <2->Paso E: etiqueta esperada $p(y|\mathbf{x},\,\theta^{k})=\dfrac{p(\mathbf{x},\,y|\theta^{k})}{\underset{y'}{\sum}p(\mathbf{x},\,y'|\theta^{k})}$
$\forall\,\mathbf{x}\in X^{u}$
\begin{itemize}
\item <2->$x$ se etiqueta de clase $c=1:2$ con proporción $p(y=c|\mathbf{x},\,\theta^{k})$.
\end{itemize}
\item <3->Paso M: MLE de $\theta$ con los ahora datos etiquetados de $X^{u}$.
\begin{itemize}
\item $w_{1:2}\rightarrow$suma de las proporciones de cada clase.
\item $\mu_{1:2}\,,\,\Sigma_{1:2}\rightarrow$media y covarianza ponderadas
de cada clase.
\end{itemize}
\item <4->Repetimos 2 y 3 hasta alcanzar un máximo local, $k=1,\,2,\,...$
\end{enumerate}
\begin{itemize}
\item <5->Es una especie de self-training.
\end{itemize}
\end{frame}
%
\begin{frame}{Pros y contras de los modelos generativos}
\begin{exampleblock}{Pros}
\begin{itemize}
\item Es un marco probabilístico claro y bien estudiado.
\item Puede ser muy efectivo si es modelo es cercano al correcto.
\end{itemize}
\end{exampleblock}
%
\begin{alertblock}{Contras}
\begin{itemize}
\item A menudo es difícil verificar la corrección del modelo.
\item La identificabilidad del modelo, no todas las distribuciones lo son
(Gauss sí, Bernouilli no).
\item EM obtiene óptimo locales no necesariamente globales.
\item Los datos no etiquetados pueden dañar si el modelo ($p(y)$ y $p(\mathbf{x}|y)$)
no es correcto.
\end{itemize}
\end{alertblock}
\end{frame}
%
\begin{frame}{Ejemplo modelo incorrecto}

Clasificación de textos por género y tema, un tema puede estar en
varios géneros.
\begin{center}
\includegraphics<2>[scale=0.85]{figs/tema_genero-1.pdf}\includegraphics<3>[scale=0.85]{figs/tema_genero-5.pdf}
\par\end{center}
\begin{itemize}
\item <4->Posibles soluciones heurísticas:
\begin{itemize}
\item <5->Construir cuidadosamente el modelo generativo: p. ej. varias
gaussianas por clase en lugar de una.
\item <5->Dar menor peso a los datos no etiquetados (${\color{red}\lambda<1}$)\\
$\log p(X_{l},Y_{l},X_{u}|\theta)=\stackrel[i=1]{l}{\sum}\log p(y_{i}|\theta)p(\mathbf{x}_{i}|y_{i},\theta)+$\\
\hspace*{0.22\paperwidth}${\color{red}{\color{black}{\color{red}\lambda}\stackrel[i=l+1]{l+u}{\sum}\log\left(\stackrel[y=1]{2}{\sum}p(y|\theta)p(\mathbf{x}_{i}|y,\theta)\right)}}$.
\end{itemize}
\end{itemize}
\end{frame}
%
\begin{frame}{Variante: cluster-and-label}
\begin{itemize}
\item <1->Cluster-and-label es una variante discriminativa, en lugar de
usar modelos probabilísticos usa algoritmos de clustering.
\end{itemize}
%
\begin{columns}

\column{0.60\textwidth}
\begin{itemize}
\item <2->Entradas $(x_{1},y_{1}),...,(x_{l},y_{l}),$ $x_{l+1},...,x_{l+u}$,
un alg. de clustering $\mathcal{A}$ y un alg. de clasificación SL
$\mathcal{L}$.
\begin{enumerate}
\item <3->Agrupar $x_{1},...,x_{l},x_{l+1},...,x_{l+u}$ usando $\mathcal{A}$.
\item <4->Para cada cluster sea $S$ sus datos etiquetados.
\item <5->Entrenar un clasificador SL para $S$, $f_{S}=\mathcal{L}(S)$.
\item <6->Aplicar $f_{S}$ a no etiquetados de $S$, así obtenemos $y_{l+1},...,y_{l+u}$.
\end{enumerate}
\end{itemize}

\column{.40\textwidth}

\includegraphics<2->[scale=0.45]{figs/cluster_and_lable-proc-5.pdf}\llap{\includegraphics<2>[scale=0.45]{figs/cluster_and_lable-proc-10.pdf}}\llap{\includegraphics<3-4>[scale=0.45]{figs/cluster_and_lable-proc-15.pdf}}\llap{\includegraphics<5->[scale=0.45]{figs/cluster_and_lable-proc-20.pdf}}
\end{columns}

\end{frame}
%
\begin{frame}{C-and-L puede funcionar o no}

$\mathcal{A}$ clustering jerárquico, $\mathcal{L}$ voto de mayoría
\end{frame}
%
\begin{frame}{Pros y contras del cluster-and-label}
\begin{exampleblock}{Pros}
\begin{itemize}
\item Funciona bastante bien cuando la hipótesis de los clusters se cumple
y se elige el algoritmo de clustering apropiado.
\end{itemize}
\end{exampleblock}
%
\begin{alertblock}{Contras}
\begin{itemize}
\item Si el algoritmo tiene muchos parámetros puede no ser aplicable en
aplicaciones reales.
\end{itemize}
\end{alertblock}
\end{frame}

\subsection{Semi-supervised SVM (S3VM)}

\subsection{Modelos basados en grafos}

\AtBeginSection{}

\section*{Bibliografía}
\begin{frame}[allowframebreaks]{Bibliografía}
\begin{itemize}
\item \htmladdnormallink {Hands-on Machine Learning with Scikit-Learn and TensorFlow, Aurélien Géron} {http://proquest.safaribooksonline.com.ezproxy.uned.es/book/programming/9781491962282} 
\end{itemize}
\end{frame}
\ThankYouFrame
\end{document}
